---
title: "Análisis de documentos - Text Mining"
author: "Salvador Carrillo Fuentes"
date: "Mayo de 2019"
output: html_document
---

## *Text Mining*

### Introducción

Es un área de ciencia de datos que se encarga de tomar grandes cantidades de lenguaje no estructurado y extraer útil e innovador conocimiento que ayude en la toma de decisiones.

* La extracción de conocimiento en documentos textuales es uno de los tópicos de investigación más activos.

* Las técnicas de *Text-Mining* pueden ayudar a las empresas en la mejora de productos, los servicios a sus consumidores, los procesos de marketing y de recursos humanos.

* Ejemplos de textos importantes a analizar: *Pdfs*, informes, artículos de investigación de un tema concreto, presentaciones de resultados, informes de prensa, revisiones de prensa, discursos políticos, análisis de textos legales, búsquedas automáticas en webs (*crawlers*), etc.

* Técnicas: análisis de frecuencias de palabras, de patrones, de frases, búsqueda de relaciones entre palabras, de patrones o de frases, etc.

### ¿Porqué usar *Text-Mining*?

* La gran mayoría de las empresas tratan con datos de tipo categórico o numérico.

* No es lógico usar sólo datos estructurados, destacando el esfuerzo que supone para la empresa el extraer conocimiento de muy diverdas fuentes no estructuradas (documentos, webs, etc) para conseguir una entrada estructurada.

* El texto representa una entrada inexplorada que puede dar a la empresa una ventaja competitiva respecto a sus competidores.

* En un mundo hiper-competitivo, una empresa debe estar atenta y ser sensible con la opinión de sus usuarios. El tratamiento de la inmensa cantidad de opiniones que se vierte en las Redes Sociales debe automatizarse y *Text-Mining* puede ser la herramienta a elegir.

### Etapas en *Text-Mining* como herramienta de análisis

* Definir el problema y objetivos específicos.
* Identificar las fuentes de los textos a coleccionar.
* Organizar el texto.
* Extraer características.
* Analizar.
* Extraer conocimiento para realizar conclusiones y/o recomendaciones.

### Fases

* Importar los datos a una estructura que llamaremos ***corpus***.
* Manipular ***corpus***.
* Preprocesamiento - ***stem the words***.
* Analizar palabras simples, o ***bigrams of words*** (grupo de dos palabras adyacentes) o ***trigrams*** (tres palabras adyacentes), ***n-grams***.
* Construir la matriz de términos-documentos.

### Tipos de *Text-Mining*

* **Bolsas de palabras** (*bag of words*): Es el más usado por su simplicidad. Fácil de comprender y de aplicar. Recolección de palabras más frecuentes.

* **Párser sintáctico**: Tiene en cuenta el orden de las palabras y el tipo gramatical de las palabras.

### Bag of words

Trata cada palabra o grupo de palabras (*n-grams*) como una caractéristica única en el documento. Encaja con las técnicas habituales de *Machine-Learning* puesto que obtiene una matriz de de observaciones y palabras con frecuencias.

Dos tipos de matrices: **DTM** (*document term matrix*) y **TDM** (*Term document matrix*).

### Ejemplo DTM

Dado un conjunto de tweets acerca de un tema determinado, su *DTM* sería:

Tweet | @user1 | @user2 | @user3 | word1 | word2 | word3 | word4 | ...
----- | ------ | ------ | ------ | ----- | ----- | ----- | ----- | ---
  1 	|   1 	 |   0 	  |   0 	 |   2   |	 1   |	 1 	 |   0   |
  2   |   0    |   1 	  |   0 	 |   1 	 |   0   |   2   |	 0 	 |
  3   |   0    |   0 	  |   1    |	 1   |   0   |	 0 	 |   1   |

### Ejemplo TDM

Dado un conjunto de tweets acerca de un tema determinado, su *TDM* sería:

Word    | Tweet1 | Twee2 | Twee3 
------- | ------ | ----- | -----
@user1 	|   1 	 |   0 	 |   0 
@user2  |   0    |   1 	 |   0 	 
@user3  |   0    |   0 	 |   1    
word1   |   2    |   1   |   1
word2   |   1    |   0   |   0
word3   |   1    |   2   |   0
...     |        |       |

### Fases de Preprocesamiento

* Descargar textos e importar en *R*.
* Buscar ***stop-words*** (listas de palabras vacías) dependiendo del idioma.
* Manipulación de los metadatos: eliminar palabras extremadamente raras, patrones no interesantes, símbolos extraños, etc.
* En general todo lo que no interese depende del análisis que estemos haciendo. etc
* A veces las palabras extremadamente raras son las interesantes para buscar conocimiento oculto muy extraño que es interesante por otro lado (aplicaciones en E-learning - buscar patrones de respuestas de alumnos a cuestionarios, exámenes que no son los habituales - muy malas o muy malas respuestas).
* **Regla interesante**: Eliminar palabras con $tfid$ menor que un valor umbral:

      $tfid = f_{ij}\times log\frac{n}{d_j}$

donde $n$ es el número de documentos, $d_j$ el número de documentos conteniendo la palabra $j$, $f_{ij}$ es la frecuencia relativa de la palabra $j$ en documento $i$ (respecto del total en documento $i$).

### Paquetes para tratamiento de enormes cantidades de documentos

En caso de que el número de documentos, *tweets*, etc. sea enorme, el uso de la estructura de datos *data.frame* puede provocar problemas por las limitaciones de memoria *RAM*.

En ese caso, usar paquetes *data.table* o *SOAR*.


